# ðŸ”¬ Research Agent â€” Human-in-the-Loop Research Assistant

An AI-powered research agent built with **LangGraph** that interprets ambiguous user queries, validates its understanding through a human-in-the-loop confirmation step, and generates structured research reports â€” all orchestrated as a formal state machine.

---

## ðŸŽ¯ Goal

Users rarely express research needs with full precision. A direct "query â†’ output" pipeline produces results that may miss the user's actual intent. This project solves that by inserting an **Interpret â†’ Validate â†’ Research** loop:

1. The AI **analyzes** the user's vague input and fills in gaps with reasonable assumptions.
2. It **presents** a structured interpretation and pauses for user confirmation.
3. If the user corrects, the AI **loops back** and re-analyzes with the new context.
4. Once confirmed, it generates a **comprehensive research report**.

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ANALYZE    â”‚â”€â”€â”€â”€â–¶â”‚  PRESENT & VALIDATEâ”‚â”€â”€â”€â”€â–¶â”‚  RESEARCH/OUTPUT â”‚
â”‚  (AI thinks) â”‚     â”‚  (User confirms)   â”‚     â”‚  (Final action)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            (User corrects)
```

### State Machine Flow

```
START â†’ analyze_node â†’ present_node (interrupt â¸) â†’ classify_node
                                                        â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ CONFIRMED   â”‚ CORRECTED    â”‚
                                          â–¼             â–¼              â”‚
                                    research_node   analyze_node â—„â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                         END
```

---

## ðŸ“‚ Project Structure

```
backend/
â”œâ”€â”€ app.py                          # Entry point (FastAPI â€” TODO)
â”œâ”€â”€ main.py                         # Alt entry point
â”œâ”€â”€ pyproject.toml                  # Dependencies (managed by uv)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example                    # Template for API keys
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py                   # Loads API keys from .env
    â”œâ”€â”€ constants.py                # Model names, temperatures, roles
    â”‚
    â””â”€â”€ Research_Agent/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py
        â”‚
        â”œâ”€â”€ state/
        â”‚   â””â”€â”€ state.py            # State (TypedDict) + InterpretedContext (Pydantic)
        â”‚
        â”œâ”€â”€ nodes/
        â”‚   â”œâ”€â”€ analyze_node.py     # Phase 1: Structured interpretation via LLM
        â”‚   â”œâ”€â”€ present_node.py     # Phase 2: interrupt() for human validation
        â”‚   â”œâ”€â”€ classify_node.py    # Phase 3: LLM classifies user reply
        â”‚   â”œâ”€â”€ research_node.py    # Phase 4: Final research report generation
        â”‚   â””â”€â”€ basic_chatbot_node.py  # Legacy echo node (unused)
        â”‚
        â”œâ”€â”€ graph/
        â”‚   â””â”€â”€ graph_builder.py    # Wires nodes + edges, compiles with MemorySaver
        â”‚
        â”œâ”€â”€ LLMS/
        â”‚   â”œâ”€â”€ groqllm.py          # ChatGroq factory (primary)
        â”‚   â””â”€â”€ geminillm.py        # Gemini LLM factory (alternate)
        â”‚
        â”œâ”€â”€ tools/
        â”‚   â””â”€â”€ search_tool.py      # TavilySearchResults (optional web enrichment)
        â”‚
        â””â”€â”€ testing/
            â””â”€â”€ contextBuilder.py   # Standalone prototype script (reference only)
```

---

## ðŸ§© Core Components

### State (`state/state.py`)

| Field | Type | Purpose |
|---|---|---|
| `raw_input` | `str` | User's original query |
| `messages` | `List[dict]` | Chat history (append-only) |
| `interpreted_context` | `InterpretedContext` | Pydantic model â€” domain, goal, assumptions, confidence |
| `gathered_data` | `List[str]` | Research output chunks (append-only) |
| `is_confirmed` | `bool` | Set to `True` when user confirms interpretation |
| `iteration_count` | `int` | Number of analyze â†’ validate loops completed |
| `user_corrections` | `List[str]` | Corrections fed back into re-analysis (append-only) |

### Nodes

| Node | File | What It Does |
|---|---|---|
| **analyze** | `analyze_node.py` | Uses `with_structured_output()` to parse user input into an `InterpretedContext` Pydantic model |
| **present** | `present_node.py` | Formats the interpretation as a summary and calls `interrupt()` to pause for user input |
| **classify** | `classify_node.py` | LLM classifies user reply as `CONFIRMED`, `CORRECTED`, or `REJECTED` |
| **research** | `research_node.py` | Generates a structured report; optionally enriched with Tavily search results |

### Graph (`graph/graph_builder.py`)

- Uses `StateGraph(State)` from LangGraph
- `MemorySaver` checkpoint enables `interrupt()` / `Command(resume=...)` across requests
- Conditional edge after `classify_node` drives the correction loop
- `compiled_graph` is a module-level singleton â€” built once, shared across all requests

---

## âš™ï¸ Tech Stack

| Category | Technology |
|---|---|
| Graph Orchestration | LangGraph â‰¥ 1.0.8 |
| LLM Provider | Groq Cloud (`llama-3.3-70b-versatile`) |
| Structured Output | Pydantic v2 + `with_structured_output()` |
| Web Search (optional) | Tavily via `langchain-community` |
| Checkpointing | LangGraph `MemorySaver` (in-memory) |
| Package Manager | `uv` |
| Python | â‰¥ 3.11 |

---

## ðŸš€ Getting Started

```bash
# 1. Clone and navigate
cd backend

# 2. Create .env from the template
cp .env.example .env
# Fill in: GROQ_API_KEY (required), TAVILY_API_KEY (optional)

# 3. Install dependencies
uv sync

# 4. Verify the graph compiles
uv run python -c "from src.Research_Agent.graph.graph_builder import compiled_graph; print('OK')"
```

---

## ðŸ”® Planned Features (Not Yet Implemented)

> The following features are part of the roadmap but **not present in the current codebase**.

| Feature | Description |
|---|---|
| **FastAPI + React Frontend** | REST API (`/chat/start`, `/chat/resume`) with a React chat UI replacing the prototype Streamlit demo |
| **Dynamic Agent Creation** | Ability to spin up specialized sub-agents on the fly based on the research domain detected during analysis |
| **Multi-Agent Collaboration** | Architect, Red Team, and Blue Team agents working together to produce validated research outputs |
| **Persistent Checkpointing** | Replace `MemorySaver` with `SqliteSaver` or `PostgresSaver` for session persistence across server restarts |
| **Iteration Cap** | Hard limit on the analyze â†’ validate loop (3â€“5 iterations) to prevent infinite correction cycles |
| **Streaming Responses** | Server-Sent Events (SSE) for real-time token-by-token output in the frontend |
| **Authentication** | User login with per-user session management and chat history |

---

## ðŸ“„ License

This project is for academic / personal use.
